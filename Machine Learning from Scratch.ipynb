{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning from Scratch\n",
    "\n",
    "Let's build some models without using libraries like sklearn!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** TABLE OF CONTENTS TO COME LATER **"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Some libraries we might want at some point "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import datasets\n",
    "from inspect import getmembers\n",
    "from collections import OrderedDict, Counter\n",
    "import numbers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First we need data...\n",
    "\n",
    "Note: this notebook is strictly about building common ML models from simple code so we're not going to do justice to any dataset by properly examining it. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Iris Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "iris = datasets.load_iris() # data imported from sklearn\n",
    "flower_type = []\n",
    "for i in iris.target:\n",
    "        flower_type.append(iris.target_names[i])       \n",
    "data = iris.data[:,:2]\n",
    "labels = flower_type\n",
    "train_data, test_data, train_labels, test_labels = train_test_split(data, labels, test_size=.20, random_state=25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='section3_1'></a>\n",
    "\n",
    "## k-Nearest Neighbor (k-NN)\n",
    "\n",
    "k-NN is the idea that you can predict the correct label for a data point based on the labels of it's closest neighbors. If you wanted to know how much it would cost to ship a package from location X to Florida and you only knew the cost of shipping from X to California and X to Georgia... probably assume that since Georgia is so close to Florida it would cost something simliar to that shipping price and ignore the cost of shipping to California. This is k-NN, where k is the number of neighbors to consider when making a decision. Consider our shipping situation again, but this time imagine we have way more data. We know the price to ship from location X to any city in the U.S. except Tampa Bay, Florida. If we use k-NN with k = 1, we would look for the city closest to Tampa Bay and decide that whatever the shipping rate is there, it will be the same for Tampa Bay. However, if we want k to equal 3, we would look at the three closest cities to Tampa Bay and choose the shipping cost most common to them. For instance if two of the three cities cost \\$5 to ship and for a third city the cost is \\$3, we will choose the majority cost of \\$5 as the shipping fee to Tampa Bay. \n",
    "\n",
    "KNN is a **non-parametric** and **lazy** algorithm. These ideas are somewhat related in that you may expect a \"lazy\" algorithm would be non-parametric. \n",
    "\n",
    "The algorithm is non-parametric because the hyperparameter K, changes the number of parameters the model will use. The number of parameters must be fixed in the model for it to be parametric. \n",
    "\n",
    "KNN algorithm is \"lazy\" because there is no actual training phase. Most other algorithms take a set of training data, learn some sort of formula for how best to classify, regress, etc. (optimize whatever objective function) the data. Then you come along with a new data point for the trained algorithm to make a prediction on. When it makes the prediction, it doesn't require the training data any longer, it has already _learned_ what it will from the training data. However, with KNN there is no actual training phase, the \"training\" happens at **inference**; KNN maps the new data point of interest to where it should best fit amongst the training data you gave it. Effectively instead of doing any \"training\" or \"learning\" it just memorized the training data and maps new values inside the training data. Ultimately the takeaway is that the algorithm doesn't have to do anything until you want a prediction, so we call it lazy.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementing the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class KNN_Classifier():\n",
    "\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def euclidean_distance(self, x, y):\n",
    "        if len(x) != len(y):\n",
    "            raise Exception(\"Values in dataset don't have the same dimensions.\")\n",
    "        distance = 0.0\n",
    "        for i in range(len(x)):\n",
    "            distance += (x[i] - y[i])**2\n",
    "        distance = np.sqrt(distance)\n",
    "        return distance\n",
    "\n",
    "    def manhattan_distance(self, x, y):\n",
    "        if len(x) != len(y):\n",
    "            raise Exception(\"Values in dataset don't have the same dimensions.\")\n",
    "        distance = 0\n",
    "        for i in range(len(x)):\n",
    "            distance += np.abs(x[i] - y[i])\n",
    "        return distance    \n",
    "\n",
    "    def predict(self, k, labeled_train_points, test_points, distance=\"euclidean\"):\n",
    "        \n",
    "        predictions = []\n",
    "        \n",
    "        # if there's only one data point to make a prediction on\n",
    "        if (isinstance(test_points[0], numbers.Number)):\n",
    "            test_points = [test_points] \n",
    "            \n",
    "        for i in range(len(test_points)):\n",
    "            if distance == \"euclidean\":\n",
    "                by_distance = sorted(labeled_train_points, key=lambda data: self.euclidean_distance(data[0], test_points[i]))\n",
    "            elif distance == \"manhattan\":\n",
    "                by_distance = sorted(labeled_train_points, key=lambda data: self.manhattan_distance(data[0], test_points[i]))\n",
    "                \n",
    "            k_nearest_labels = [label for _, label in by_distance[:k]] \n",
    "            \n",
    "            while(True):\n",
    "                vote_counts = Counter(k_nearest_labels)\n",
    "                winner, winner_count = vote_counts.most_common(1)[0]\n",
    "\n",
    "                num_winners = len([count for count in vote_counts.values() if count == winner_count])\n",
    "                if num_winners == 1:\n",
    "                    predictions.append(winner)\n",
    "                    break\n",
    "                else:\n",
    "                    k_nearest_labels = k_nearest_labels[:-1]\n",
    "\n",
    "        return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "labeled_train_data = list(zip(train_data, train_labels))\n",
    "k_val = 2\n",
    "knn_model = KNN_Classifier()\n",
    "hg_predictions = knn_model.predict(k=k_val, labeled_train_points=labeled_train_data, test_points=test_data, distance=\"manhattan\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "acc_score = 0\n",
    "for i in range(len(hg_predictions)):\n",
    "    if hg_predictions[i] == test_labels[i]:\n",
    "        acc_score += 1\n",
    "acc_score /= len(hg_predictions)\n",
    "\n",
    "print(\"Accuracy of homegrown k-NN model with k = {}: {:.2f}%\".format( k_val, (acc_score*100)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from matplotlib.colors import ListedColormap\n",
    "\n",
    "n_neighbors = 1\n",
    "\n",
    "# import some data to play with\n",
    "iris = datasets.load_iris()\n",
    "\n",
    "# we only take the first two features. We could avoid this ugly\n",
    "# slicing by using a two-dim dataset\n",
    "X = iris.data[:, :2]\n",
    "y = iris.target\n",
    "\n",
    "h = .02  # step size in the mesh\n",
    "\n",
    "# Create color maps\n",
    "cmap_light = ListedColormap(['#FFAAAA', '#AAFFAA', '#AAAAFF'])\n",
    "cmap_bold = ListedColormap(['#FF0000', '#00FF00', '#0000FF'])\n",
    "\n",
    "\n",
    "# we create an instance of Neighbours Classifier and fit the data.\n",
    "clf = KNeighborsClassifier(n_neighbors)\n",
    "clf.fit(X, y)\n",
    "\n",
    "# Plot the decision boundary. For that, we will assign a color to each\n",
    "# point in the mesh [x_min, x_max]x[y_min, y_max].\n",
    "x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
    "y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
    "xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n",
    "                     np.arange(y_min, y_max, h))\n",
    "Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "\n",
    "# Put the result into a color plot\n",
    "Z = Z.reshape(xx.shape)\n",
    "plt.figure(figsize=(8,8))\n",
    "plt.pcolormesh(xx, yy, Z, cmap=cmap_light)\n",
    "\n",
    "# Plot also the training points\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y, cmap=cmap_bold,\n",
    "            edgecolor='k', s=20)\n",
    "plt.xlim(xx.min(), xx.max())\n",
    "plt.ylim(yy.min(), yy.max())\n",
    "plt.title(\"k-NN Classification of Iris Dataset (k = %i)\"\n",
    "          % (n_neighbors))\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What you're looking at:\n",
    "\n",
    "The three regions of background color represent the decision boundaries of our k-NN model while the dots represent the actual data we provided to make those boundaries. \n",
    "\n",
    "Notice how some of the dots are not in the right k-NN decision boundary? Why do you think that is (hint: consider how the decision boundary would be different for k=1 vs. k=2)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
